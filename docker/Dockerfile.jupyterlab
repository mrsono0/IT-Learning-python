FROM mrsono0/base_project:jupyter_alpine_3_9

RUN echo "http://dl-3.alpinelinux.org/alpine/v3.9/main" > /etc/apk/repositories && \
    echo "http://dl-3.alpinelinux.org/alpine/v3.9/community" >> /etc/apk/repositories && \
    echo "http://dl-3.alpinelinux.org/alpine/edge/testing" >> /etc/apk/repositories && \
    echo "http://apk.nbgallery.s3-us-west-2.amazonaws.com/alpine38" >> /etc/apk/repositories
RUN apk update && \
    apk upgrade &&\
    apk add build-base openjdk8

ENV JAVA_INSTALL_PATH /usr/lib/jvm/java-8-openjdk-amd64
RUN	ln -sf "$JAVA_INSTALL_PATH" /usr/lib/jvm/default-jvm; \
	ln -sf "$JAVA_INSTALL_PATH" /usr/lib/jvm/latest
RUN cd /usr/local/bin \
    && ln -sf "${JAVA_HOME}"/bin/java java \
    && ln -sf "${JAVA_HOME}"/bin/javac javac

# RUN /opt/conda/bin/conda install conda python=3.6.8 --yes 
RUN conda update -n base -c defaults conda -y

ENV CONDA_PACKAGES="\
    pyyaml \
    cffi \
    h5py \
    pytest \
    coverage \
    pylint \
    pygments \
    cryptography \
    pexpect \
    tornado \
    jinja2 \
    mkl \
    Flask \
    Flask-SQLAlchemy \
    pillow \
    graphviz \
    seaborn \
    matplotlib \
    tqdm \
    pymysql \
    sqlalchemy \
    paramiko \
    statsmodels \
    Cython \
    pyspark \
    scipy \
    scikit-learn \
    missingno \
    plotnine \
    folium \
    "
# RUN /opt/conda/bin/conda install ${CONDA_PACKAGES} python=3.6.8 --yes && \
RUN /opt/conda/bin/conda install -c conda-forge ${CONDA_PACKAGES} --yes

ENV PIP_PACKAGES="\
    Flask-Script \
    Flask-Migrate \
    Flask-CLI \
    flask-batch \
    torch \
    xgboost \
    folium \
    progressbar \
    torchvision \
    mxnet-mkl \
    koalanlp \
    # tensorflow \
    # keras  \
    jupyter_c_kernel \
    " 
RUN pip install -U -v ${PIP_PACKAGES}

RUN conda install -c conda-forge \
    cling \
    xeus-cling \
    blaze \
    py4j 
RUN conda clean --yes --all

WORKDIR /root
RUN git clone https://github.com/kakao/khaiii.git
RUN mkdir /root/khaiii/build
WORKDIR /root/khaiii/build

RUN cmake ..
RUN make all
RUN make resource
# RUN make large_resource
RUN make install
RUN make package_python
WORKDIR /root/khaiii/build/package_python
RUN pip install .
# from koalanlp.Util import initialize
# 꼬꼬마와 ETRI 분석기의 2.0.4 버전을 참조합니다.
# initialize(java_options="-Xmx4g", KKMA="2.0.4", ETRI="2.0.4")

# RUN export PYSPARK_DRIVER_PYTHON="jupyter"  >> /etc/profile
# RUN export PYSPARK_PYTHON=python3  >> /etc/profile
# RUN echo "export PYSPARK_DRIVER_PYTHON_OPTS='notebook --notebook-dir=/workspace --port=8888 --no-browser --allow-root --ip=0.0.0.0 --NotebookApp.token='" >> /etc/profiles
# RUN sed -i "s/PYSPARK_DRIVER_PYTHON=ipython/PYSPARK_DRIVER_PYTHON=jupyter/g" /etc/profile
# RUN echo 'export PYSPARK_DRIVER_PYTHON_OPTS="lab --notebook-dir=/workspace --port=8888 --no-browser --allow-root --ip=0.0.0.0 --NotebookApp.token="'  >> /etc/profile

# RUN WORKDIR/usr/local/share/jupyter/kernels/pyspark
# RUN mkdir -p /usr/local/share/jupyter/kernels/pyspark/
# RUN echo '{' > /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '  "display_name": "PySpark", ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '  "language": "python", ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '  "argv": [ "/opt/conda/bin/python", "-m", "ipykernel", "-f", "{connection_file}" ], ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '  "env": { ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '    "SPARK_HOME": "/usr/spark-2.2.0", ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '    "PYSPARK_PYTHON": "/opt/conda/bin/python", ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '    "PYTHONPATH": "/usr/spark/python/python/:/usr/spark/python/lib/py4j-0.9-src.zip", ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '    "PYTHONSTARTUP": "/usr/spark/python/pyspark/shell.py", ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '    "PYSPARK_SUBMIT_ARGS": "--master yarn-client pyspark-shell" ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '  } ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json
# RUN echo '} ' >> /usr/local/share/jupyter/kernels/pyspark/kernel.json

WORKDIR /workspace